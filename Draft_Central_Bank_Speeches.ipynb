{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP process\n",
    "1. Segmentation\n",
    "2. Tokenizing\n",
    "3. Stop words\n",
    "4. Stemming (-ing -s -ed)\n",
    "5. Lemmatization(am are is : be)\n",
    "6. Speech Tagging (noun, verb, preposition...)\n",
    "7. Named Entity Tagging (location, name...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " # dir\n",
    "work_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(work_dir, \"INPUT/all_speeches.csv\")\n",
    "speeches_data = pd.read_csv(input_path)\n",
    "speeches_data[\"date\"] = pd.to_datetime(speeches_data[\"date\"],format=\"%d/%m/%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select 100 row for test\n",
    "#df_raw = speeches_data.sample(n = 100)\n",
    "\n",
    "# selected latest 20 row for test\n",
    "df_raw = speeches_data.sort_values(\"date\").tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 keyBERT: key words extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer Tips & Tricks\n",
    "1. vectorizer = CountVectorizer(ngram_range=(1, 3), stop_words=\"english\")\n",
    "2. use KeyphraseVectorizers: \\\n",
    "Extract grammatically accurate keyphases based on their part-of-speech tags.\\\n",
    "No need to specify n-gram ranges.\\\n",
    "Get document-keyphrase matrices.\\\n",
    "Multiple language support.\\\n",
    "User-defined part-of-speech patterns for keyphrase extraction possible.\n",
    "3. leverage the MMR function on top of KeyBERT to diversify the output\n",
    "4. part-of-speech\\\n",
    "KeyphraseVectorizers extracts the part-of-speech tags from the documents and then applies a regex pattern to extract keyphrases that fit within that pattern. \\\n",
    "\n",
    "Embedding models:\n",
    "1. Sentence Transformers\n",
    "2. Flair\n",
    "3. Spacy\n",
    "4. Universal Sentence Encoder (USE)\n",
    "5. Gensim¶\n",
    "Note that Gensim is primarily used for Word Embedding models. This works typically best for short documents since the word embeddings are pooled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "#!pip install keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install keyphrase_vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improvement:\n",
    "def keyBERT_word(text):   # CountVectorizer \n",
    "    \n",
    "    kw_model = KeyBERT(model=\"all-MiniLM-L6-v2\")\n",
    "    #kw_model = KeyBERT()\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 1), stop_words=\"english\")\n",
    "    keywords = kw_model.extract_keywords(\n",
    "                      doc=text,\n",
    "                      vectorizer=vectorizer,\n",
    "                      # use_maxsum=True,\n",
    "                      use_mmr=True, \n",
    "                      # diversity=0.7,\n",
    "                      # nr_candidates=20, \n",
    "                      top_n=top_n)\n",
    "  \n",
    "    li_keywords = [pair[0] for pair in keywords] # keywords list without similarity value\n",
    "\n",
    "    return li_keywords\n",
    "\n",
    "\n",
    "\n",
    "def keyBERT_phase(text):   # KeyphraseVectorizers\n",
    "    \n",
    "    #kw_model = KeyBERT()\n",
    "    kw_model = KeyBERT(model=\"all-MiniLM-L6-v2\")\n",
    "    vectorizer= KeyphraseCountVectorizer()\n",
    "    keywords = kw_model.extract_keywords(\n",
    "                        docs=text, \n",
    "                        vectorizer=vectorizer,\n",
    "                        use_mmr=True)\n",
    "    \n",
    "    li_keywords = [pair[0] for pair in keywords] # keywords list without similarity value\n",
    "    \n",
    "    return li_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords_01 = df_raw.copy()\n",
    "#1. compare keyphrase_ngram_range of 1,2,3\n",
    "df_keywords_01[\"keywords\"] = df_keywords_01[\"text\"].apply(lambda x: keyBERT_word(x）)\n",
    "df_keywords_01[\"keyphase\"] = df_keywords_01[\"text\"].apply(lambda x: keyBERT_phase(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>country</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>is_gov</th>\n",
       "      <th>text</th>\n",
       "      <th>keywords_1</th>\n",
       "      <th>keywords_2</th>\n",
       "      <th>keywords_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3221</th>\n",
       "      <td>r220620b_ECB</td>\n",
       "      <td>euro area</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>NO_INFO</td>\n",
       "      <td>lane</td>\n",
       "      <td>0</td>\n",
       "      <td>Notes: The vertical line indicates the start o...</td>\n",
       "      <td>[hicp, forecast, gdp, hicpx, forecasts]</td>\n",
       "      <td>[hicpx quarterly, model hicp, hicp energy, hic...</td>\n",
       "      <td>[hicp energy prices, hicp hicpx quarterly, hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6114</th>\n",
       "      <td>r220620a_BOE</td>\n",
       "      <td>united kingdom</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>UK monetary policy in the context of global sp...</td>\n",
       "      <td>mann</td>\n",
       "      <td>0</td>\n",
       "      <td>Welcome to this presentation of the May . The ...</td>\n",
       "      <td>[inflation, inflationary, shocks, monetary, ec...</td>\n",
       "      <td>[shocks russia, shocks global, supply shocks, ...</td>\n",
       "      <td>[large shocks russia, shocks russia invasion, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3220</th>\n",
       "      <td>r220620a_ECB</td>\n",
       "      <td>euro area</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>Hearing of the Committee on Economic and Monet...</td>\n",
       "      <td>lagarde</td>\n",
       "      <td>1</td>\n",
       "      <td>It is a pleasure to be here again for our seco...</td>\n",
       "      <td>[euro, eurosystem, monetary, brussels, sanctions]</td>\n",
       "      <td>[affecting euro, monetary policy, relevant eur...</td>\n",
       "      <td>[facing monetary policy, severely affecting eu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>r220621a_BOA</td>\n",
       "      <td>australia</td>\n",
       "      <td>2022-06-21</td>\n",
       "      <td>Inflation and Monetary Policy</td>\n",
       "      <td>lowe</td>\n",
       "      <td>1</td>\n",
       "      <td>I would like to thank AMCHAM for the invitatio...</td>\n",
       "      <td>[inflation, inflationary, monetary, yield, cpi]</td>\n",
       "      <td>[current inflationary, ongoing inflation, rece...</td>\n",
       "      <td>[responding higher inflation, inflation austra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3222</th>\n",
       "      <td>r220622a_ECB</td>\n",
       "      <td>euro area</td>\n",
       "      <td>2022-06-22</td>\n",
       "      <td>Good, bad and hopeful news: the latest on the ...</td>\n",
       "      <td>elderson</td>\n",
       "      <td>0</td>\n",
       "      <td>I understand that today's audience includes ma...</td>\n",
       "      <td>[ecb, risks, risk, crises, climate]</td>\n",
       "      <td>[risks climate, risks banks, practices ecb, cl...</td>\n",
       "      <td>[risk management ecb, risks practices ecb, ban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>r221102a_BOC</td>\n",
       "      <td>canada</td>\n",
       "      <td>2022-11-02</td>\n",
       "      <td>Preparing for payments supervision</td>\n",
       "      <td>morrow</td>\n",
       "      <td>0</td>\n",
       "      <td>Good morning, and thank you for inviting me to...</td>\n",
       "      <td>[banks, bank, fintech, payments, payment]</td>\n",
       "      <td>[payments fintech, bank canada, finance canada...</td>\n",
       "      <td>[bank canada evolving, payments ecosystem cana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3245</th>\n",
       "      <td>r221103a_ECB</td>\n",
       "      <td>euro area</td>\n",
       "      <td>2022-11-03</td>\n",
       "      <td>Mind the step: calibrating monetary policy in ...</td>\n",
       "      <td>panetta</td>\n",
       "      <td>0</td>\n",
       "      <td>The euro area is facing a sequence of unpreced...</td>\n",
       "      <td>[inflationary, inflation, euro, eurosystem, mo...</td>\n",
       "      <td>[inflation euro, risks inflation, risks euro, ...</td>\n",
       "      <td>[euro reinforcing inflationary, risks inflatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3246</th>\n",
       "      <td>r221104b_ECB</td>\n",
       "      <td>euro area</td>\n",
       "      <td>2022-11-04</td>\n",
       "      <td>The euro area economy and the energy transition</td>\n",
       "      <td>guindos</td>\n",
       "      <td>0</td>\n",
       "      <td>I am very pleased to be taking part in this ev...</td>\n",
       "      <td>[inflation, macroeconomic, inflationary, econo...</td>\n",
       "      <td>[energy inflation, economy following, economy ...</td>\n",
       "      <td>[inflation rising energy, euro area economy, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3247</th>\n",
       "      <td>r221104a_ECB</td>\n",
       "      <td>euro area</td>\n",
       "      <td>2022-11-04</td>\n",
       "      <td>Monetary policy in a high inflation environmen...</td>\n",
       "      <td>lagarde</td>\n",
       "      <td>1</td>\n",
       "      <td>Inflation in the euro area is far too high, re...</td>\n",
       "      <td>[inflation, inflationary, monetary, recessions...</td>\n",
       "      <td>[estonia inflation, exacerbate inflationary, i...</td>\n",
       "      <td>[inflation entrenched euro, estonia inflation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4948</th>\n",
       "      <td>r221110a_SNB</td>\n",
       "      <td>switzerland</td>\n",
       "      <td>2022-11-10</td>\n",
       "      <td>Current challenges to central banks' independence</td>\n",
       "      <td>jordan</td>\n",
       "      <td>1</td>\n",
       "      <td>Ladies and gentlemen I am very grateful for th...</td>\n",
       "      <td>[macroeconomic, economists, economics, banks, ...</td>\n",
       "      <td>[central banks, banks independence, bank indep...</td>\n",
       "      <td>[central banks independence, central bank inde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         reference         country       date  \\\n",
       "3221  r220620b_ECB       euro area 2022-06-20   \n",
       "6114  r220620a_BOE  united kingdom 2022-06-20   \n",
       "3220  r220620a_ECB       euro area 2022-06-20   \n",
       "279   r220621a_BOA       australia 2022-06-21   \n",
       "3222  r220622a_ECB       euro area 2022-06-22   \n",
       "...            ...             ...        ...   \n",
       "901   r221102a_BOC          canada 2022-11-02   \n",
       "3245  r221103a_ECB       euro area 2022-11-03   \n",
       "3246  r221104b_ECB       euro area 2022-11-04   \n",
       "3247  r221104a_ECB       euro area 2022-11-04   \n",
       "4948  r221110a_SNB     switzerland 2022-11-10   \n",
       "\n",
       "                                                  title    author  is_gov  \\\n",
       "3221                                            NO_INFO      lane       0   \n",
       "6114  UK monetary policy in the context of global sp...      mann       0   \n",
       "3220  Hearing of the Committee on Economic and Monet...   lagarde       1   \n",
       "279                       Inflation and Monetary Policy      lowe       1   \n",
       "3222  Good, bad and hopeful news: the latest on the ...  elderson       0   \n",
       "...                                                 ...       ...     ...   \n",
       "901                  Preparing for payments supervision    morrow       0   \n",
       "3245  Mind the step: calibrating monetary policy in ...   panetta       0   \n",
       "3246    The euro area economy and the energy transition   guindos       0   \n",
       "3247  Monetary policy in a high inflation environmen...   lagarde       1   \n",
       "4948  Current challenges to central banks' independence    jordan       1   \n",
       "\n",
       "                                                   text  \\\n",
       "3221  Notes: The vertical line indicates the start o...   \n",
       "6114  Welcome to this presentation of the May . The ...   \n",
       "3220  It is a pleasure to be here again for our seco...   \n",
       "279   I would like to thank AMCHAM for the invitatio...   \n",
       "3222  I understand that today's audience includes ma...   \n",
       "...                                                 ...   \n",
       "901   Good morning, and thank you for inviting me to...   \n",
       "3245  The euro area is facing a sequence of unpreced...   \n",
       "3246  I am very pleased to be taking part in this ev...   \n",
       "3247  Inflation in the euro area is far too high, re...   \n",
       "4948  Ladies and gentlemen I am very grateful for th...   \n",
       "\n",
       "                                             keywords_1  \\\n",
       "3221            [hicp, forecast, gdp, hicpx, forecasts]   \n",
       "6114  [inflation, inflationary, shocks, monetary, ec...   \n",
       "3220  [euro, eurosystem, monetary, brussels, sanctions]   \n",
       "279     [inflation, inflationary, monetary, yield, cpi]   \n",
       "3222                [ecb, risks, risk, crises, climate]   \n",
       "...                                                 ...   \n",
       "901           [banks, bank, fintech, payments, payment]   \n",
       "3245  [inflationary, inflation, euro, eurosystem, mo...   \n",
       "3246  [inflation, macroeconomic, inflationary, econo...   \n",
       "3247  [inflation, inflationary, monetary, recessions...   \n",
       "4948  [macroeconomic, economists, economics, banks, ...   \n",
       "\n",
       "                                             keywords_2  \\\n",
       "3221  [hicpx quarterly, model hicp, hicp energy, hic...   \n",
       "6114  [shocks russia, shocks global, supply shocks, ...   \n",
       "3220  [affecting euro, monetary policy, relevant eur...   \n",
       "279   [current inflationary, ongoing inflation, rece...   \n",
       "3222  [risks climate, risks banks, practices ecb, cl...   \n",
       "...                                                 ...   \n",
       "901   [payments fintech, bank canada, finance canada...   \n",
       "3245  [inflation euro, risks inflation, risks euro, ...   \n",
       "3246  [energy inflation, economy following, economy ...   \n",
       "3247  [estonia inflation, exacerbate inflationary, i...   \n",
       "4948  [central banks, banks independence, bank indep...   \n",
       "\n",
       "                                             keywords_3  \n",
       "3221  [hicp energy prices, hicp hicpx quarterly, hea...  \n",
       "6114  [large shocks russia, shocks russia invasion, ...  \n",
       "3220  [facing monetary policy, severely affecting eu...  \n",
       "279   [responding higher inflation, inflation austra...  \n",
       "3222  [risk management ecb, risks practices ecb, ban...  \n",
       "...                                                 ...  \n",
       "901   [bank canada evolving, payments ecosystem cana...  \n",
       "3245  [euro reinforcing inflationary, risks inflatio...  \n",
       "3246  [inflation rising energy, euro area economy, e...  \n",
       "3247  [inflation entrenched euro, estonia inflation ...  \n",
       "4948  [central banks independence, central bank inde...  \n",
       "\n",
       "[100 rows x 10 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path = os.path.join(work_dir, \"OUTPUT/all_speeches_BERT.csv\")\n",
    "df_keywords_01.sort_values([\"country\",\"date\"]).to_csv(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2. GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.summary of full text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.summary of paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. text clustering: document level, sentence level, word level\n",
    "\n",
    "1. 每一条speech对应的一组keywords也可以vectorization ,对每一个speech 聚类\n",
    "2. 可以直接算document-level vector similarity吗？ 是否有可比性？文章长度限制？ （比较关键词更好？）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jiayue.yuan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Libraries for preprocessing\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "#import webcolors\n",
    "\n",
    "#Download once if using NLTK for preprocessing\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "#Libraries for vectorisation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#from fuzzywuzzy import fuzz\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#Libraries for clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!!!!!!!!!!!!!!!!!!!!!\n",
    "# Load output data if restart the kenel \n",
    "work_dir = os.getcwd()\n",
    "output_path = os.path.join(work_dir, \"OUTPUT/all_speeches_BERT.csv\")\n",
    "df_keywords_01 = pd.read_csv(output_path)\n",
    "df_keywords_01['date'] = pd.to_datetime(df_keywords_01[\"date\"])\n",
    "\n",
    "def str2li(str):\n",
    "    li = str.strip('][\\'').replace(\"'\",\"\").split(', ')\n",
    "    return li\n",
    "    \n",
    "df_keywords_01['keywords_1'] = df_keywords_01['keywords_1'].apply(str2li)\n",
    "df_keywords_01['keywords_2'] = df_keywords_01['keywords_2'].apply(str2li)\n",
    "df_keywords_01['keywords_3'] = df_keywords_01['keywords_3'].apply(str2li)\n",
    "df_keywords_01['keywords'] = df_keywords_01['keywords'].apply(str2li)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>reference</th>\n",
       "      <th>country</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>is_gov</th>\n",
       "      <th>text</th>\n",
       "      <th>keywords_1</th>\n",
       "      <th>keywords_2</th>\n",
       "      <th>keywords_3</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3221</td>\n",
       "      <td>r220620b_ECB</td>\n",
       "      <td>euro area</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>NO_INFO</td>\n",
       "      <td>lane</td>\n",
       "      <td>0</td>\n",
       "      <td>Notes: The vertical line indicates the start o...</td>\n",
       "      <td>[hicp, forecast, gdp, hicpx, forecasts]</td>\n",
       "      <td>[hicpx quarterly, model hicp, hicp energy, hic...</td>\n",
       "      <td>[hicp energy prices, hicp hicpx quarterly, hea...</td>\n",
       "      <td>[hicp energy prices, future oil demand, econom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6114</td>\n",
       "      <td>r220620a_BOE</td>\n",
       "      <td>united kingdom</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>UK monetary policy in the context of global sp...</td>\n",
       "      <td>mann</td>\n",
       "      <td>0</td>\n",
       "      <td>Welcome to this presentation of the May . The ...</td>\n",
       "      <td>[inflation, inflationary, shocks, monetary, ec...</td>\n",
       "      <td>[shocks russia, shocks global, supply shocks, ...</td>\n",
       "      <td>[large shocks russia, shocks russia invasion, ...</td>\n",
       "      <td>[domestic inflationary pressures, demand shock...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3220</td>\n",
       "      <td>r220620a_ECB</td>\n",
       "      <td>euro area</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>Hearing of the Committee on Economic and Monet...</td>\n",
       "      <td>lagarde</td>\n",
       "      <td>1</td>\n",
       "      <td>It is a pleasure to be here again for our seco...</td>\n",
       "      <td>[euro, eurosystem, monetary, brussels, sanctions]</td>\n",
       "      <td>[affecting euro, monetary policy, relevant eur...</td>\n",
       "      <td>[facing monetary policy, severely affecting eu...</td>\n",
       "      <td>[monetary policy, monetary policy meeting, eur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>279</td>\n",
       "      <td>r220621a_BOA</td>\n",
       "      <td>australia</td>\n",
       "      <td>2022-06-21</td>\n",
       "      <td>Inflation and Monetary Policy</td>\n",
       "      <td>lowe</td>\n",
       "      <td>1</td>\n",
       "      <td>I would like to thank AMCHAM for the invitatio...</td>\n",
       "      <td>[inflation, inflationary, monetary, yield, cpi]</td>\n",
       "      <td>[current inflationary, ongoing inflation, rece...</td>\n",
       "      <td>[responding higher inflation, inflation austra...</td>\n",
       "      <td>[higher inflation, inflation rate, inflation s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3222</td>\n",
       "      <td>r220622a_ECB</td>\n",
       "      <td>euro area</td>\n",
       "      <td>2022-06-22</td>\n",
       "      <td>Good, bad and hopeful news: the latest on the ...</td>\n",
       "      <td>elderson</td>\n",
       "      <td>0</td>\n",
       "      <td>I understand that today's audience includes ma...</td>\n",
       "      <td>[ecb, risks, risk, crises, climate]</td>\n",
       "      <td>[risks climate, risks banks, practices ecb, cl...</td>\n",
       "      <td>[risk management ecb, risks practices ecb, ban...</td>\n",
       "      <td>[eu climate targets, climate risks, climate cr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     reference         country       date  \\\n",
       "0        3221  r220620b_ECB       euro area 2022-06-20   \n",
       "1        6114  r220620a_BOE  united kingdom 2022-06-20   \n",
       "2        3220  r220620a_ECB       euro area 2022-06-20   \n",
       "3         279  r220621a_BOA       australia 2022-06-21   \n",
       "4        3222  r220622a_ECB       euro area 2022-06-22   \n",
       "\n",
       "                                               title    author  is_gov  \\\n",
       "0                                            NO_INFO      lane       0   \n",
       "1  UK monetary policy in the context of global sp...      mann       0   \n",
       "2  Hearing of the Committee on Economic and Monet...   lagarde       1   \n",
       "3                      Inflation and Monetary Policy      lowe       1   \n",
       "4  Good, bad and hopeful news: the latest on the ...  elderson       0   \n",
       "\n",
       "                                                text  \\\n",
       "0  Notes: The vertical line indicates the start o...   \n",
       "1  Welcome to this presentation of the May . The ...   \n",
       "2  It is a pleasure to be here again for our seco...   \n",
       "3  I would like to thank AMCHAM for the invitatio...   \n",
       "4  I understand that today's audience includes ma...   \n",
       "\n",
       "                                          keywords_1  \\\n",
       "0            [hicp, forecast, gdp, hicpx, forecasts]   \n",
       "1  [inflation, inflationary, shocks, monetary, ec...   \n",
       "2  [euro, eurosystem, monetary, brussels, sanctions]   \n",
       "3    [inflation, inflationary, monetary, yield, cpi]   \n",
       "4                [ecb, risks, risk, crises, climate]   \n",
       "\n",
       "                                          keywords_2  \\\n",
       "0  [hicpx quarterly, model hicp, hicp energy, hic...   \n",
       "1  [shocks russia, shocks global, supply shocks, ...   \n",
       "2  [affecting euro, monetary policy, relevant eur...   \n",
       "3  [current inflationary, ongoing inflation, rece...   \n",
       "4  [risks climate, risks banks, practices ecb, cl...   \n",
       "\n",
       "                                          keywords_3  \\\n",
       "0  [hicp energy prices, hicp hicpx quarterly, hea...   \n",
       "1  [large shocks russia, shocks russia invasion, ...   \n",
       "2  [facing monetary policy, severely affecting eu...   \n",
       "3  [responding higher inflation, inflation austra...   \n",
       "4  [risk management ecb, risks practices ecb, ban...   \n",
       "\n",
       "                                            keywords  \n",
       "0  [hicp energy prices, future oil demand, econom...  \n",
       "1  [domestic inflationary pressures, demand shock...  \n",
       "2  [monetary policy, monetary policy meeting, eur...  \n",
       "3  [higher inflation, inflation rate, inflation s...  \n",
       "4  [eu climate targets, climate risks, climate cr...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_keywords_01.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all keywords into a list\n",
    "li_keywords = []\n",
    "for i in df_keywords_01[\"keywords_1\"]:\n",
    "    for k in i:\n",
    "        li_keywords.append(k)\n",
    "\n",
    "# 1)Removing stopwords (punctuation and numbers)\n",
    "li_keywords_nonstop = [remove_stopwords(x) for x in li_keywords]\n",
    "\n",
    "# 2)Stemming and making words lower case （remove）\n",
    "#li_keywords_stemmed = [PorterStemmer().stem(word) for word in li_keywords_nonstop]\n",
    "\n",
    "# 3)dedup\n",
    "li_keywords_clean = list(set(li_keywords_nonstop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "184\n"
     ]
    }
   ],
   "source": [
    "print(len(li_keywords_stemmed))\n",
    "print(len(li_keywords_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['overseeing',\n",
       " 'critic',\n",
       " 'bundesbank',\n",
       " 'forecasting',\n",
       " 'banknote',\n",
       " 'findings',\n",
       " 'longevity',\n",
       " 'assets',\n",
       " 'economists',\n",
       " 'documents',\n",
       " 'cpi',\n",
       " 'insurer',\n",
       " 'inflationary',\n",
       " 'merchants',\n",
       " 'payment',\n",
       " 'mortgage',\n",
       " 'stakeholders',\n",
       " 'tribal',\n",
       " 'lisbon',\n",
       " 'award',\n",
       " 'conference',\n",
       " 'risk',\n",
       " 'governors',\n",
       " 'environmental',\n",
       " 'studies',\n",
       " 'guidance',\n",
       " 'unemployment',\n",
       " 'brussels',\n",
       " 'yield',\n",
       " 'stablecoins',\n",
       " 'euro',\n",
       " 'insurance',\n",
       " 'fiscal',\n",
       " 'unemployed',\n",
       " 'easing',\n",
       " 'hicpx',\n",
       " 'biodiversity',\n",
       " 'speech',\n",
       " 'macroeconomics',\n",
       " 'decades',\n",
       " 'sustainable',\n",
       " 'regulation',\n",
       " 'hicp',\n",
       " 'policies',\n",
       " 'information',\n",
       " 'policymakers',\n",
       " 'innovation',\n",
       " 'kansas',\n",
       " 'investment',\n",
       " 'rate',\n",
       " 'disruptions',\n",
       " 'canadians',\n",
       " 'european',\n",
       " 'pandemic',\n",
       " 'disturbances',\n",
       " 'reinsurance',\n",
       " 'friedman',\n",
       " 'energy',\n",
       " 'macroeconomic',\n",
       " 'cryptocurrency',\n",
       " 'massachusetts',\n",
       " 'eurobarometer',\n",
       " 'housing',\n",
       " 'crises',\n",
       " 'lenders',\n",
       " 'currencies',\n",
       " 'hurricane',\n",
       " 'tightening',\n",
       " 'ecb',\n",
       " 'currency',\n",
       " 'economy',\n",
       " 'rates',\n",
       " 'demand',\n",
       " 'belfast',\n",
       " 'economist',\n",
       " 'glasgow',\n",
       " 'businesses',\n",
       " 'deposits',\n",
       " 'mortgages',\n",
       " 'anika',\n",
       " 'cryptoasset',\n",
       " 'yen',\n",
       " 'workforce',\n",
       " 'equity',\n",
       " 'decentralized',\n",
       " 'qe',\n",
       " 'rural',\n",
       " 'ireland',\n",
       " 'prospects',\n",
       " 'brunner',\n",
       " 'analytics',\n",
       " 'snb',\n",
       " 'eurosystem',\n",
       " 'confidentiality',\n",
       " 'welshman',\n",
       " 'blockchains',\n",
       " 'blockchain',\n",
       " 'banking',\n",
       " 'treasury',\n",
       " 'savings',\n",
       " 'finance',\n",
       " 'ecosystem',\n",
       " 'reforms',\n",
       " 'analysed',\n",
       " 'fintech',\n",
       " 'commerce',\n",
       " 'participation',\n",
       " 'forecasts',\n",
       " 'recession',\n",
       " 'cryptocurrencies',\n",
       " 'franc',\n",
       " 'wages',\n",
       " 'households',\n",
       " 'economies',\n",
       " 'gdp',\n",
       " 'economic',\n",
       " 'nationalbank',\n",
       " 'japan',\n",
       " 'consultation',\n",
       " 'educators',\n",
       " 'crisis',\n",
       " 'governor',\n",
       " 'risks',\n",
       " 'scotland',\n",
       " 'fed',\n",
       " 'consumption',\n",
       " 'recessionary',\n",
       " 'estimates',\n",
       " 'regulatory',\n",
       " 'sanctions',\n",
       " 'bitcoin',\n",
       " 'spillovers',\n",
       " 'markets',\n",
       " 'payments',\n",
       " 'portfolios',\n",
       " 'education',\n",
       " 'firms',\n",
       " 'crypto',\n",
       " 'assessed',\n",
       " 'covid',\n",
       " 'interests',\n",
       " 'basel',\n",
       " 'securities',\n",
       " 'bis',\n",
       " 'nature',\n",
       " 'fluctuating',\n",
       " 'eu',\n",
       " 'climate',\n",
       " 'hobart',\n",
       " 'tribes',\n",
       " 'students',\n",
       " 'aging',\n",
       " 'bank',\n",
       " 'inflation',\n",
       " 'retailers',\n",
       " 'community',\n",
       " 'geopolitical',\n",
       " 'insurers',\n",
       " 'financial',\n",
       " 'analyses',\n",
       " 'educational',\n",
       " 'houses',\n",
       " 'stability',\n",
       " 'monetary',\n",
       " 'iosco',\n",
       " 'welcoming',\n",
       " 'market',\n",
       " 'irish',\n",
       " 'economics',\n",
       " 'criticism',\n",
       " 'ftf',\n",
       " 'banks',\n",
       " 'recessions',\n",
       " 'bankers',\n",
       " 'shocks',\n",
       " 'analysing',\n",
       " 'disclosure',\n",
       " 'liquidity',\n",
       " 'forecast',\n",
       " 'mayor',\n",
       " 'lender',\n",
       " 'keynesian',\n",
       " 'ecommerce',\n",
       " 'mayors']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li_keywords_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 embedding\n",
    "\n",
    "1: trained a model using all speeches(BERT) and embedding selected key words/key phases \\\n",
    "2: use a pre-trained model(Word2Vector/BERT), load the full dictionary and convert each keyword to vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) KeyBERT.extract_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_model = KeyBERT()\n",
    "doc_embeddings, word_embeddings = kw_model.extract_embeddings(li_keywords_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m4/71qzhz0j3h78_rcdbd9k0kzw0000gp/T/ipykernel_9926/4086555835.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# ?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(word_embeddings) # ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Word2Vec (gensim.models)\n",
    "\n",
    "### Word2Vec VS BERT\n",
    "\n",
    "Architecture: Word2Vec is a shallow neural network with a single hidden layer, while BERT is a deep transformer network.\n",
    "\n",
    "Inputs: Word2Vec takes a single word as input and predicts the context words, while BERT takes a sequence of words as input and predicts the masked words.\n",
    "\n",
    "Pretraining: Word2Vec is trained on a large corpus of text to learn word representations, while BERT is trained on a large corpus of text as well as on specific NLP tasks (such as question answering and sentiment analysis).\n",
    "\n",
    "Transfer Learning: Word2Vec is often used as a feature in NLP models and provides fixed-length vectors that can be plugged into other models, while BERT can be fine-tuned on specific NLP tasks with minimal architecture changes, allowing it to adapt to new data and improve performance.\n",
    "\n",
    "Performance: BERT generally outperforms Word2Vec on various NLP tasks, such as text classification, named entity recognition, and question answering. However, Word2Vec can still be useful in some cases where computational resources are limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model first\n",
    "#w = Word2Vec(,vector_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "# Load the dictionary of a pre-trained model\n",
    "import gensim.downloader\n",
    "# Show all available models in gensim-data\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v_vectors = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.38943 ,  0.10575 , -1.2358  , -0.61433 ,  0.25176 ,  0.066561,\n",
       "       -0.50221 , -1.9261  ,  0.91188 ,  0.33808 ,  1.075   ,  0.3934  ,\n",
       "       -1.8223  ,  1.558   ,  0.81213 , -1.3259  , -0.58039 ,  0.72888 ,\n",
       "        0.93991 , -0.62783 , -0.46672 ,  0.35953 ,  1.0572  , -0.054854,\n",
       "       -0.86242 ], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "glove_model = api.load(\"glove-twitter-25\")\n",
    "glove_model['inflation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.04150391, -0.05224609, -0.34179688,  0.76171875,  0.015625  ,\n",
       "       -0.11572266,  0.04199219, -0.24414062,  0.14160156, -0.36132812,\n",
       "        0.05810547, -0.18066406,  0.22265625,  0.28710938, -0.47070312,\n",
       "        0.52734375,  0.40039062, -0.04248047,  0.08984375, -0.11816406,\n",
       "        0.3671875 ,  0.33398438,  0.53515625,  0.53125   ,  0.0112915 ,\n",
       "       -0.29882812,  0.31054688, -0.00506592,  0.28320312,  0.21582031,\n",
       "        0.09033203, -0.7421875 , -0.13964844, -0.33203125, -0.29882812,\n",
       "       -0.30078125,  0.07910156,  0.09619141, -0.09667969,  0.59375   ,\n",
       "        0.07470703, -0.13378906,  0.5703125 , -0.19824219, -0.26953125,\n",
       "        0.02832031,  0.38085938,  0.19140625, -0.18164062, -0.11376953,\n",
       "        0.24121094,  0.28320312, -0.08398438, -0.10205078,  0.4453125 ,\n",
       "       -0.02380371,  0.03491211,  0.23535156,  0.07666016,  0.05200195,\n",
       "        0.19921875,  0.01165771,  0.09570312, -0.03637695,  0.140625  ,\n",
       "        0.15917969, -0.07275391,  0.27148438,  0.203125  ,  0.16503906,\n",
       "       -0.06396484, -0.15136719,  0.33007812, -0.01422119, -0.17871094,\n",
       "        0.06347656,  0.18554688,  0.17773438,  0.07421875, -0.02087402,\n",
       "        0.0222168 ,  0.04760742, -0.32617188,  0.20996094,  0.07324219,\n",
       "       -0.078125  , -0.21972656,  0.13476562,  0.12109375, -0.06396484,\n",
       "        0.11376953, -0.1171875 , -0.31054688, -0.00811768, -0.43554688,\n",
       "       -0.38476562,  0.34179688, -0.06542969,  0.30859375, -0.18066406,\n",
       "       -0.0402832 , -0.03613281,  0.06152344, -0.08056641,  0.09277344,\n",
       "       -0.05493164, -0.2890625 ,  0.41796875,  0.1640625 , -0.06689453,\n",
       "       -0.11914062, -0.34375   ,  0.08544922,  0.4140625 , -0.09619141,\n",
       "        0.21679688,  0.09082031, -0.03125   ,  0.01385498, -0.16699219,\n",
       "       -0.12792969,  0.20898438, -0.25      , -0.00338745,  0.00110626,\n",
       "        0.04736328,  0.0402832 , -0.09179688, -0.47265625, -0.4765625 ,\n",
       "        0.10107422, -0.26953125, -0.0111084 , -0.06884766, -0.11132812,\n",
       "        0.52734375,  0.15820312, -0.17675781, -0.08740234,  0.578125  ,\n",
       "        0.1171875 , -0.26367188, -0.34765625,  0.12792969, -0.3046875 ,\n",
       "       -0.29296875,  0.14648438,  0.20019531,  0.00198364, -0.06542969,\n",
       "       -0.23632812,  0.02575684,  0.31054688,  0.34765625,  0.12353516,\n",
       "       -0.01452637, -0.421875  , -0.23339844,  0.11230469, -0.3203125 ,\n",
       "       -0.37890625, -0.11035156,  0.23339844, -0.17382812, -0.25390625,\n",
       "        0.14941406, -0.09912109,  0.58984375,  0.16308594,  0.16796875,\n",
       "       -0.5703125 ,  0.1953125 , -0.06933594,  0.29492188, -0.00595093,\n",
       "       -0.07421875, -0.12597656, -0.3515625 ,  0.07666016, -0.28515625,\n",
       "       -0.265625  , -0.31054688,  0.19042969, -0.4140625 , -0.00921631,\n",
       "       -0.3125    , -0.01916504,  0.83984375,  0.19140625,  0.21777344,\n",
       "       -0.28125   ,  0.11816406, -0.29492188, -0.18164062, -0.00100708,\n",
       "        0.29882812, -0.07763672,  0.125     , -0.11621094, -0.33398438,\n",
       "       -0.00093842, -0.03759766, -0.09130859, -0.14746094,  0.02905273,\n",
       "       -0.2109375 ,  0.08837891, -0.10400391,  0.04931641, -0.00860596,\n",
       "       -0.05175781, -0.38085938,  0.31445312,  0.12597656, -0.29882812,\n",
       "       -0.06201172, -0.03320312, -0.05786133, -0.04541016,  0.11035156,\n",
       "       -0.15722656,  0.20507812, -0.23632812, -0.09863281,  0.08154297,\n",
       "        0.01184082, -0.00157166,  0.14648438,  0.10644531,  0.25976562,\n",
       "       -0.34765625, -0.140625  ,  0.43359375,  0.16015625, -0.16113281,\n",
       "        0.16015625,  0.62890625, -0.1796875 ,  0.29296875, -0.43359375,\n",
       "        0.12109375,  0.17480469, -0.07958984,  0.11572266, -0.26367188,\n",
       "       -0.07421875, -0.07666016, -0.35351562,  0.05151367, -0.2578125 ,\n",
       "       -0.1328125 , -0.22753906, -0.34765625,  0.04956055,  0.08447266,\n",
       "       -0.11425781,  0.012146  , -0.36523438,  0.12597656,  0.04321289,\n",
       "        0.21191406, -0.06933594,  0.43945312, -0.15039062,  0.09033203,\n",
       "        0.12304688, -0.31640625, -0.56640625,  0.29882812,  0.13671875,\n",
       "        0.17871094,  0.25      , -0.00531006, -0.03295898,  0.08740234,\n",
       "        0.25390625,  0.24609375, -0.06396484,  0.00637817,  0.01586914,\n",
       "        0.03393555, -0.05224609,  0.28320312,  0.0071106 ,  0.08300781,\n",
       "       -0.33398438,  0.10498047,  0.05200195, -0.00976562,  0.21972656,\n",
       "       -0.21875   , -0.17675781,  0.22460938,  0.01635742, -0.14648438,\n",
       "        0.07421875,  0.03027344,  0.078125  ,  0.07519531, -0.24511719],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model = api.load(\"word2vec-google-news-300\")\n",
    "w2v_model['inflation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'bundesbank' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m4/71qzhz0j3h78_rcdbd9k0kzw0000gp/T/ipykernel_9926/146843228.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvec_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mli_keywords_clean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m# not in dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mvec_dic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_dic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \"\"\"\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \"\"\"\n\u001b[0;32m--> 438\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'bundesbank' not present\""
     ]
    }
   ],
   "source": [
    "vec_dic = {}\n",
    "for word in li_keywords_clean:\n",
    "    vector = w2v_model[word]   # not in dictionary\n",
    "    # exception pass\n",
    "    vec_dic[word] = vector\n",
    "pd.DataFrame(vec_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3)  SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SentenceTransformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m4/71qzhz0j3h78_rcdbd9k0kzw0000gp/T/ipykernel_9875/2165540178.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrans_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"all-MiniLM-L6-v2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#embeddings = trans_model.encode(sentences)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inflation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SentenceTransformer' is not defined"
     ]
    }
   ],
   "source": [
    "trans_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "#embeddings = trans_model.encode(sentences)\n",
    "len(trans_model.encode(\"inflation\"))\n",
    "\n",
    "# kernel die"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 PCA&k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming and making words lower case\n",
    "\n",
    "# kmeans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Speech labelling\n",
    "\n",
    "可以直接用pre-trained model, 选择topic word, 然后print most similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
