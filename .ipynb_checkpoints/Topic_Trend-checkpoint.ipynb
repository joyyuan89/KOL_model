{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f38eefd",
   "metadata": {},
   "source": [
    "# 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9630e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# dir\n",
    "work_dir = os.getcwd()\n",
    "input_path = os.path.join(work_dir, \"INPUT/all_speeches.csv\")\n",
    "speeches_data = pd.read_csv(input_path)\n",
    "speeches_data[\"date\"] = pd.to_datetime(speeches_data[\"date\"],format=\"%d/%m/%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d459a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected latest 20 row for test\n",
    "df_raw = speeches_data.set_index(\"date\").tail(20)\n",
    "\n",
    "# phase 2.0 weights by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4130b86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.9/site-packages (1.0.2)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.1-cp39-cp39-macosx_10_9_x86_64.whl (9.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.21.5)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.9.1)\n",
      "Installing collected packages: joblib, scikit-learn\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.1.0\n",
      "    Uninstalling joblib-1.1.0:\n",
      "      Successfully uninstalled joblib-1.1.0\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.2\n",
      "    Uninstalling scikit-learn-1.0.2:\n",
      "      Successfully uninstalled scikit-learn-1.0.2\n",
      "Successfully installed joblib-1.2.0 scikit-learn-1.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1354ec6",
   "metadata": {},
   "source": [
    "# 2. Keyword Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6ba27d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
    "\n",
    "\n",
    "# embedding\n",
    "\n",
    "\n",
    "# 2.1 vectorization\n",
    "def CountVectorizer_func(doc,ngram_range = (1,1)):\n",
    "\n",
    "    count = CountVectorizer(ngram_range= ngram_range, stop_words=\"english\").fit([doc])\n",
    "    candidates = count.get_feature_names_out()\n",
    "    \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d11e653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['candidate', 'candidate keywords', 'creating', 'creating list',\n",
       "       'document', 'keyphrases', 'keyphrases document', 'keywords',\n",
       "       'keywords keyphrases', 'list', 'list candidate', 'start',\n",
       "       'start creating'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = \"We start by creating a list of candidate keywords or keyphrases from a document.\"\n",
    "CountVectorizer_func(doc,(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2b38159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['candidate', 'creating', 'document', 'keyphrases', 'keywords',\n",
       "       'list', 'start'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorizer_func(doc,(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8645746f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('keyphrases document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2ebbcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
